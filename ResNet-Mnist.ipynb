{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet on Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import sys\n",
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log(log_file_path, string):\n",
    "    '''\n",
    "    Write one line of log into screen and file.\n",
    "        log_file_path: Path of log file.\n",
    "        string:        String to write in log file.\n",
    "    '''\n",
    "    with open(log_file_path, 'a+') as f:\n",
    "        f.write(string + '\\n')\n",
    "        f.flush()\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_dir = '/scratch/f1fan/ResNet/data/'\n",
    "train_image_f = 'train-images-idx3-ubyte.gz'\n",
    "train_label_f = 'train-labels-idx1-ubyte.gz'\n",
    "test_image_f = 't10k-images-idx3-ubyte.gz'\n",
    "test_label_f = 't10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read32(bytestream):\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    img_f = os.path.join(mnist_dir, train_image_f)\n",
    "    lbl_f = os.path.join(mnist_dir, train_label_f)\n",
    "    \n",
    "    with gzip.open(img_f) as img_bytestream, gzip.open(lbl_f) as lbl_bytestream:\n",
    "        # Check magic number\n",
    "        magic_img, magic_lbl = _read32(img_bytestream), _read32(lbl_bytestream)\n",
    "        if magic_img != 2051 or magic_lbl != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "        \n",
    "        # Read shape\n",
    "        image_cnt, label_cnt = _read32(img_bytestream), _read32(lbl_bytestream)\n",
    "        rows = _read32(img_bytestream)\n",
    "        cols = _read32(img_bytestream)\n",
    "        \n",
    "        # Read label\n",
    "        label_buf = lbl_bytestream.read(label_cnt)\n",
    "        labels = np.frombuffer(label_buf, dtype=np.uint8)\n",
    "        \n",
    "        # Read image\n",
    "        image_buf = img_bytestream.read(rows * cols * image_cnt)\n",
    "        images = np.frombuffer(image_buf, dtype=np.uint8)\n",
    "        images = images.reshape(image_cnt, rows, cols, 1)\n",
    "        \n",
    "        return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    img_f = os.path.join(mnist_dir, test_image_f)\n",
    "    lbl_f = os.path.join(mnist_dir, test_label_f)\n",
    "    \n",
    "    with gzip.open(img_f) as img_bytestream, gzip.open(lbl_f) as lbl_bytestream:\n",
    "        # Check magic number\n",
    "        magic_img, magic_lbl = _read32(img_bytestream), _read32(lbl_bytestream)\n",
    "        if magic_img != 2051 or magic_lbl != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "        \n",
    "        # Read shape\n",
    "        image_cnt, label_cnt = _read32(img_bytestream), _read32(lbl_bytestream)\n",
    "        rows = _read32(img_bytestream)\n",
    "        cols = _read32(img_bytestream)\n",
    "        \n",
    "        # Read label\n",
    "        label_buf = lbl_bytestream.read(label_cnt)\n",
    "        labels = np.frombuffer(label_buf, dtype=np.uint8)\n",
    "        \n",
    "        # Read image\n",
    "        image_buf = img_bytestream.read(rows * cols * image_cnt)\n",
    "        images = np.frombuffer(image_buf, dtype=np.uint8)\n",
    "        images = images.reshape(image_cnt, rows, cols, 1)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_per_pixel_mean(train_images, test_images):\n",
    "    images = np.concatenate((train_images, test_images), axis=0)\n",
    "    return np.mean(images, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = load_train_data()\n",
    "test_images, test_labels = load_test_data()\n",
    "\n",
    "#train_images = train_images - pp_mean\n",
    "#train_images = train_images / 128.0\n",
    "#test_images = test_images - pp_mean\n",
    "#test_images = test_images / 128.0\n",
    "\n",
    "train_images = 1.0 * train_images / 255 * 2.0 - 1.0\n",
    "test_images = 1.0 * test_images / 255 * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage 1: Load training and testing images...\n"
     ]
    }
   ],
   "source": [
    "exp_id = 2\n",
    "num_units = 5\n",
    "epoch = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.0002  \n",
    "\n",
    "log_file_path = os.path.join('/scratch/f1fan/ResNet', \"log_exp{}.txt\".format(exp_id))  \n",
    "log(log_file_path,\n",
    "    \"Training stage 1: Load training and testing images...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_layer, output_channels, filter_size, strides, scope):    \n",
    "    with tf.variable_scope(scope):\n",
    "        # Variable for filter.\n",
    "        in_channels = input_layer.get_shape().as_list()[-1]\n",
    "    \n",
    "        conv_filter = tf.get_variable(\n",
    "            name = 'filter', \n",
    "            shape = [filter_size, filter_size, in_channels, output_channels],\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(seed = 777),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale = 0.0002)\n",
    "        )\n",
    "        # Do convolution.\n",
    "        conv = tf.nn.conv2d(input_layer, \n",
    "                            conv_filter, \n",
    "                            strides = strides, \n",
    "                            padding = 'SAME')\n",
    "        # Variable for bias.\n",
    "        bias = tf.get_variable(name = 'bias', \n",
    "                               shape = [output_channels], \n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "        # Add bias.\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, bias), conv.get_shape())\n",
    "\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lrelu(input_layer, leak=0.2):\n",
    "    # Do leaky ReLU and return.\n",
    "    #return tf.maximum(input_layer, leak * input_layer)\n",
    "    return tf.nn.relu(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(input_layer, output_dim, scope):\n",
    "    shape = input_layer.get_shape().as_list()\n",
    "    batch_size, input_dim = shape\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        # Variable of weight.\n",
    "        weight = tf.get_variable(\n",
    "            name = 'weight', \n",
    "            shape = [input_dim, output_dim], \n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(seed = 777),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(0.0002)\n",
    "        )\n",
    "        \n",
    "        # Variable of bias.\n",
    "        bias = tf.get_variable(name = \"bias\", \n",
    "                               shape = [output_dim],\n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "\n",
    "        # Do multiplication and return.\n",
    "        return tf.matmul(input_layer, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(input_layer, is_training, scope, reuse):\n",
    "    return tf.contrib.layers.batch_norm(input_layer,\n",
    "                                        decay = 0.9, \n",
    "                                        updates_collections = None,\n",
    "                                        epsilon = 1e-5,\n",
    "                                        scale = True,\n",
    "                                        is_training = is_training,\n",
    "                                        reuse = reuse,\n",
    "                                        scope = scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def avg_pool(input_layer, strides, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        return tf.nn.avg_pool(input_layer, ksize=strides, strides=strides, padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def w_decay():\n",
    "    cost = []\n",
    "    for var in tf.trainable_variables():\n",
    "        if var.op.name.find(r'filter') > 0:\n",
    "            cost.append(tf.nn.l2_loss(var))\n",
    "    return tf.mul(decay_rate, tf.add_n(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def residual(input_layer, increase_dim, first, scope):\n",
    "    in_channels = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    if increase_dim:\n",
    "        out_channels = in_channels * 2\n",
    "        strides = [1, 2, 2, 1]\n",
    "    else:\n",
    "        out_channels = in_channels\n",
    "        strides = [1, 1, 1, 1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        h0 = input_layer if first else lrelu(batch_norm(input_layer, is_training=True, scope='h0_bn', reuse=False))\n",
    "        \n",
    "        h1_conv = conv2d(h0, out_channels, filter_size=3, strides=strides, scope='h1_conv')\n",
    "        h1 = lrelu(batch_norm(h1_conv, is_training=True, scope='h1_bn', reuse=False))\n",
    "        \n",
    "        h2_conv = conv2d(h1, out_channels, filter_size=3, strides=[1, 1, 1, 1], scope='h2_conv')\n",
    "        \n",
    "        if increase_dim:\n",
    "            l = avg_pool(input_layer, strides=[1, 2, 2, 1], scope='l_pool')\n",
    "            l = tf.pad(l, [[0, 0], [0, 0], \n",
    "                           [0, 0], [in_channels // 2, in_channels // 2]])\n",
    "        else:\n",
    "            l = input_layer\n",
    "\n",
    "        h2 = tf.add(h2_conv, l)\n",
    "\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resnet(images, num_units):\n",
    "    with tf.variable_scope('ResNet', reuse=False):\n",
    "        init_dim = 16\n",
    "        batch_size = images.get_shape().as_list()[0]\n",
    "        \n",
    "        r0_conv = conv2d(images, init_dim, filter_size=3, strides=[1, 1, 1, 1], scope='r0_conv')\n",
    "        r0 = lrelu(batch_norm(r0_conv, is_training=True, scope='r0_bn', reuse=False))\n",
    "        \n",
    "        r1_res = residual(r0, increase_dim=False, first=True, scope='res1.0')\n",
    "        for k in xrange(1, num_units):\n",
    "            r1_res = residual(r1_res, increase_dim=False, first=False, scope='res1.{0}'.format(k))\n",
    "\n",
    "        r2_res = residual(r1_res, increase_dim=True, first=False, scope='res2.0')\n",
    "        for k in xrange(1, num_units):\n",
    "            r2_res = residual(r2_res, increase_dim=False, first=False, scope='res2.{0}'.format(k))\n",
    "\n",
    "        r3_res = residual(r2_res, increase_dim=True, first=False, scope='res3.0')\n",
    "        for k in xrange(1, num_units):\n",
    "            r3_res = residual(r3_res, increase_dim=False, first=False, scope='res3.{0}'.format(k))\n",
    "\n",
    "        r4 = lrelu(batch_norm(r3_res, is_training=True, scope='r4_bn', reuse=False))\n",
    "        \n",
    "        axis = [1, 2]\n",
    "        r5 = tf.reduce_mean(r4, axis, name='global_pool')\n",
    "\n",
    "        fc = fully_connected(tf.reshape(r5, [batch_size, -1]), output_dim=10, scope='fully_connected')\n",
    "        return tf.nn.softmax(fc), fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_network(batch_shape):\n",
    "    # Get shape of single batch\n",
    "    [batch_size, height, width, channels] = batch_shape\n",
    "    \n",
    "    # Placeholders\n",
    "    images = tf.placeholder(dtype=tf.float32,\n",
    "                            shape=batch_shape,\n",
    "                            name='images')\n",
    "    labels = tf.placeholder(dtype=tf.int32,\n",
    "                            shape=[batch_size,],\n",
    "                            name='labels')\n",
    "    # Calculate losses\n",
    "    probability, logits = resnet(images, num_units)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                   labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    loss += w_decay()\n",
    "    \n",
    "    prediction = tf.equal(tf.cast(tf.argmax(probability, axis=1), tf.int32), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    var = [x for x in tf.trainable_variables() if 'ResNet' in x.name]\n",
    "    return [loss, var, accuracy, images, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_optimizers(loss, var):\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        global_step = tf.Variable(initial_value = 0, trainable = False)\n",
    "        global_step_op = global_step.assign_add(1)\n",
    "        \n",
    "        #boundaries = [2400, 9600, 12000]\n",
    "        boundaries = [1800, 3600, 7200]\n",
    "        values = [0.1, 0.01, 0.001, 0.0002]\n",
    "        lr = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                         momentum=0.9)\n",
    "        '''\n",
    "        global learning_rate\n",
    "        #if global_step % 600 == 0:\n",
    "        #    learning_rate = learning_rate / 2.0\n",
    "        lr = learning_rate\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        '''\n",
    "        optimizer = opt.minimize(loss=loss,\n",
    "                                 var_list=var)\n",
    "        return optimizer, global_step_op, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(sess):\n",
    "    log(log_file_path,\n",
    "        \"Training stage 2: Build network and initialize...\")\n",
    "    # Build network\n",
    "    batch_shape = [batch_size, 28, 28, 1]\n",
    "    r_loss, r_var, r_accuracy, images, labels = build_network(batch_shape)\n",
    "    r_opt, global_step_op, global_step = get_optimizers(r_loss, r_var)\n",
    "    iteration_per_epoch = train_images.shape[0] // batch_size\n",
    "    \n",
    "    # Show a list of global variables.\n",
    "    global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "    log(log_file_path, 'Global variables:')\n",
    "    for i, var in enumerate(global_variables):\n",
    "        log(log_file_path, \"{0} {1}\".format(i, var.name))\n",
    "        \n",
    "    # Initialize all variables\n",
    "    all_initializer_op = tf.global_variables_initializer()\n",
    "    sess.run(all_initializer_op)\n",
    "    \n",
    "    log(log_file_path, \"Training stage 3: Epoch training...\")\n",
    "    for i in range(epoch):\n",
    "        '''\n",
    "        schedule_idx = 0\n",
    "        schedule_epoch, schedule_lr = learning_rate_schedule[schedule_idx]\n",
    "        if i == schedule_epoch:\n",
    "            learning_rate = schedule_lr\n",
    "            schedule_idx += 1\n",
    "            print 'learning rate changed! current learning rate: ', learning_rate\n",
    "        '''\n",
    "            \n",
    "        # Shuffle training set\n",
    "        shuffle = np.random.permutation(train_images.shape[0])\n",
    "        for j in range(iteration_per_epoch):\n",
    "            # Get current batch of image\n",
    "            batch_images = train_images[shuffle[j * batch_size : (j + 1) * batch_size]]\n",
    "            batch_labels = train_labels[shuffle[j * batch_size : (j + 1) * batch_size]]\n",
    "            \n",
    "            #aug_batch_images = tf.map_fn(augment_train_image, batch_images).eval(session=sess)\n",
    "            \n",
    "            sess.run(r_opt, feed_dict = {images: batch_images, labels:batch_labels})\n",
    "            batch_loss, batch_accuracy = sess.run([r_loss, r_accuracy], \n",
    "                                                  feed_dict = {images: batch_images, labels:batch_labels})\n",
    "            if j % 50 == 0:\n",
    "                log(log_file_path, \"Training epoch {0}, iteration {1}, global_step {2}, batch_loss {3}, batch_accuracy {4}\".format(\n",
    "                    i, j, sess.run(global_step), batch_loss, batch_accuracy))\n",
    "            sess.run(global_step_op)\n",
    "        \n",
    "        test_batch_count = test_images.shape[0] // batch_size\n",
    "        test_loss = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        for k in range(test_batch_count):\n",
    "            # Get current batch of image\n",
    "            batch_images = test_images[k * batch_size : (k + 1) * batch_size]\n",
    "            batch_labels = test_labels[k * batch_size : (k + 1) * batch_size]\n",
    "            \n",
    "            #aug_batch_images = tf.map_fn(augment_test_image, batch_images).eval(session=sess)\n",
    "            \n",
    "            batch_loss, batch_accuracy = sess.run([r_loss, r_accuracy], \n",
    "                                                  feed_dict = {images: batch_images, labels:batch_labels})\n",
    "            test_loss += batch_loss\n",
    "            test_accuracy += batch_accuracy\n",
    "            \n",
    "        log(log_file_path, 'Testing epoch {0}, loss {1}, error {2}'.format(\n",
    "                i, test_loss / test_batch_count, 1.0 - (test_accuracy / test_batch_count)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage 2: Build network and initialize...\n",
      "Global variables:\n",
      "0 ResNet/r0_conv/filter:0\n",
      "1 ResNet/r0_conv/bias:0\n",
      "2 ResNet/r0_bn/beta:0\n",
      "3 ResNet/r0_bn/gamma:0\n",
      "4 ResNet/r0_bn/moving_mean:0\n",
      "5 ResNet/r0_bn/moving_variance:0\n",
      "6 ResNet/res1.0/h1_conv/filter:0\n",
      "7 ResNet/res1.0/h1_conv/bias:0\n",
      "8 ResNet/res1.0/h1_bn/beta:0\n",
      "9 ResNet/res1.0/h1_bn/gamma:0\n",
      "10 ResNet/res1.0/h1_bn/moving_mean:0\n",
      "11 ResNet/res1.0/h1_bn/moving_variance:0\n",
      "12 ResNet/res1.0/h2_conv/filter:0\n",
      "13 ResNet/res1.0/h2_conv/bias:0\n",
      "14 ResNet/res1.1/h0_bn/beta:0\n",
      "15 ResNet/res1.1/h0_bn/gamma:0\n",
      "16 ResNet/res1.1/h0_bn/moving_mean:0\n",
      "17 ResNet/res1.1/h0_bn/moving_variance:0\n",
      "18 ResNet/res1.1/h1_conv/filter:0\n",
      "19 ResNet/res1.1/h1_conv/bias:0\n",
      "20 ResNet/res1.1/h1_bn/beta:0\n",
      "21 ResNet/res1.1/h1_bn/gamma:0\n",
      "22 ResNet/res1.1/h1_bn/moving_mean:0\n",
      "23 ResNet/res1.1/h1_bn/moving_variance:0\n",
      "24 ResNet/res1.1/h2_conv/filter:0\n",
      "25 ResNet/res1.1/h2_conv/bias:0\n",
      "26 ResNet/res1.2/h0_bn/beta:0\n",
      "27 ResNet/res1.2/h0_bn/gamma:0\n",
      "28 ResNet/res1.2/h0_bn/moving_mean:0\n",
      "29 ResNet/res1.2/h0_bn/moving_variance:0\n",
      "30 ResNet/res1.2/h1_conv/filter:0\n",
      "31 ResNet/res1.2/h1_conv/bias:0\n",
      "32 ResNet/res1.2/h1_bn/beta:0\n",
      "33 ResNet/res1.2/h1_bn/gamma:0\n",
      "34 ResNet/res1.2/h1_bn/moving_mean:0\n",
      "35 ResNet/res1.2/h1_bn/moving_variance:0\n",
      "36 ResNet/res1.2/h2_conv/filter:0\n",
      "37 ResNet/res1.2/h2_conv/bias:0\n",
      "38 ResNet/res1.3/h0_bn/beta:0\n",
      "39 ResNet/res1.3/h0_bn/gamma:0\n",
      "40 ResNet/res1.3/h0_bn/moving_mean:0\n",
      "41 ResNet/res1.3/h0_bn/moving_variance:0\n",
      "42 ResNet/res1.3/h1_conv/filter:0\n",
      "43 ResNet/res1.3/h1_conv/bias:0\n",
      "44 ResNet/res1.3/h1_bn/beta:0\n",
      "45 ResNet/res1.3/h1_bn/gamma:0\n",
      "46 ResNet/res1.3/h1_bn/moving_mean:0\n",
      "47 ResNet/res1.3/h1_bn/moving_variance:0\n",
      "48 ResNet/res1.3/h2_conv/filter:0\n",
      "49 ResNet/res1.3/h2_conv/bias:0\n",
      "50 ResNet/res1.4/h0_bn/beta:0\n",
      "51 ResNet/res1.4/h0_bn/gamma:0\n",
      "52 ResNet/res1.4/h0_bn/moving_mean:0\n",
      "53 ResNet/res1.4/h0_bn/moving_variance:0\n",
      "54 ResNet/res1.4/h1_conv/filter:0\n",
      "55 ResNet/res1.4/h1_conv/bias:0\n",
      "56 ResNet/res1.4/h1_bn/beta:0\n",
      "57 ResNet/res1.4/h1_bn/gamma:0\n",
      "58 ResNet/res1.4/h1_bn/moving_mean:0\n",
      "59 ResNet/res1.4/h1_bn/moving_variance:0\n",
      "60 ResNet/res1.4/h2_conv/filter:0\n",
      "61 ResNet/res1.4/h2_conv/bias:0\n",
      "62 ResNet/res2.0/h0_bn/beta:0\n",
      "63 ResNet/res2.0/h0_bn/gamma:0\n",
      "64 ResNet/res2.0/h0_bn/moving_mean:0\n",
      "65 ResNet/res2.0/h0_bn/moving_variance:0\n",
      "66 ResNet/res2.0/h1_conv/filter:0\n",
      "67 ResNet/res2.0/h1_conv/bias:0\n",
      "68 ResNet/res2.0/h1_bn/beta:0\n",
      "69 ResNet/res2.0/h1_bn/gamma:0\n",
      "70 ResNet/res2.0/h1_bn/moving_mean:0\n",
      "71 ResNet/res2.0/h1_bn/moving_variance:0\n",
      "72 ResNet/res2.0/h2_conv/filter:0\n",
      "73 ResNet/res2.0/h2_conv/bias:0\n",
      "74 ResNet/res2.1/h0_bn/beta:0\n",
      "75 ResNet/res2.1/h0_bn/gamma:0\n",
      "76 ResNet/res2.1/h0_bn/moving_mean:0\n",
      "77 ResNet/res2.1/h0_bn/moving_variance:0\n",
      "78 ResNet/res2.1/h1_conv/filter:0\n",
      "79 ResNet/res2.1/h1_conv/bias:0\n",
      "80 ResNet/res2.1/h1_bn/beta:0\n",
      "81 ResNet/res2.1/h1_bn/gamma:0\n",
      "82 ResNet/res2.1/h1_bn/moving_mean:0\n",
      "83 ResNet/res2.1/h1_bn/moving_variance:0\n",
      "84 ResNet/res2.1/h2_conv/filter:0\n",
      "85 ResNet/res2.1/h2_conv/bias:0\n",
      "86 ResNet/res2.2/h0_bn/beta:0\n",
      "87 ResNet/res2.2/h0_bn/gamma:0\n",
      "88 ResNet/res2.2/h0_bn/moving_mean:0\n",
      "89 ResNet/res2.2/h0_bn/moving_variance:0\n",
      "90 ResNet/res2.2/h1_conv/filter:0\n",
      "91 ResNet/res2.2/h1_conv/bias:0\n",
      "92 ResNet/res2.2/h1_bn/beta:0\n",
      "93 ResNet/res2.2/h1_bn/gamma:0\n",
      "94 ResNet/res2.2/h1_bn/moving_mean:0\n",
      "95 ResNet/res2.2/h1_bn/moving_variance:0\n",
      "96 ResNet/res2.2/h2_conv/filter:0\n",
      "97 ResNet/res2.2/h2_conv/bias:0\n",
      "98 ResNet/res2.3/h0_bn/beta:0\n",
      "99 ResNet/res2.3/h0_bn/gamma:0\n",
      "100 ResNet/res2.3/h0_bn/moving_mean:0\n",
      "101 ResNet/res2.3/h0_bn/moving_variance:0\n",
      "102 ResNet/res2.3/h1_conv/filter:0\n",
      "103 ResNet/res2.3/h1_conv/bias:0\n",
      "104 ResNet/res2.3/h1_bn/beta:0\n",
      "105 ResNet/res2.3/h1_bn/gamma:0\n",
      "106 ResNet/res2.3/h1_bn/moving_mean:0\n",
      "107 ResNet/res2.3/h1_bn/moving_variance:0\n",
      "108 ResNet/res2.3/h2_conv/filter:0\n",
      "109 ResNet/res2.3/h2_conv/bias:0\n",
      "110 ResNet/res2.4/h0_bn/beta:0\n",
      "111 ResNet/res2.4/h0_bn/gamma:0\n",
      "112 ResNet/res2.4/h0_bn/moving_mean:0\n",
      "113 ResNet/res2.4/h0_bn/moving_variance:0\n",
      "114 ResNet/res2.4/h1_conv/filter:0\n",
      "115 ResNet/res2.4/h1_conv/bias:0\n",
      "116 ResNet/res2.4/h1_bn/beta:0\n",
      "117 ResNet/res2.4/h1_bn/gamma:0\n",
      "118 ResNet/res2.4/h1_bn/moving_mean:0\n",
      "119 ResNet/res2.4/h1_bn/moving_variance:0\n",
      "120 ResNet/res2.4/h2_conv/filter:0\n",
      "121 ResNet/res2.4/h2_conv/bias:0\n",
      "122 ResNet/res3.0/h0_bn/beta:0\n",
      "123 ResNet/res3.0/h0_bn/gamma:0\n",
      "124 ResNet/res3.0/h0_bn/moving_mean:0\n",
      "125 ResNet/res3.0/h0_bn/moving_variance:0\n",
      "126 ResNet/res3.0/h1_conv/filter:0\n",
      "127 ResNet/res3.0/h1_conv/bias:0\n",
      "128 ResNet/res3.0/h1_bn/beta:0\n",
      "129 ResNet/res3.0/h1_bn/gamma:0\n",
      "130 ResNet/res3.0/h1_bn/moving_mean:0\n",
      "131 ResNet/res3.0/h1_bn/moving_variance:0\n",
      "132 ResNet/res3.0/h2_conv/filter:0\n",
      "133 ResNet/res3.0/h2_conv/bias:0\n",
      "134 ResNet/res3.1/h0_bn/beta:0\n",
      "135 ResNet/res3.1/h0_bn/gamma:0\n",
      "136 ResNet/res3.1/h0_bn/moving_mean:0\n",
      "137 ResNet/res3.1/h0_bn/moving_variance:0\n",
      "138 ResNet/res3.1/h1_conv/filter:0\n",
      "139 ResNet/res3.1/h1_conv/bias:0\n",
      "140 ResNet/res3.1/h1_bn/beta:0\n",
      "141 ResNet/res3.1/h1_bn/gamma:0\n",
      "142 ResNet/res3.1/h1_bn/moving_mean:0\n",
      "143 ResNet/res3.1/h1_bn/moving_variance:0\n",
      "144 ResNet/res3.1/h2_conv/filter:0\n",
      "145 ResNet/res3.1/h2_conv/bias:0\n",
      "146 ResNet/res3.2/h0_bn/beta:0\n",
      "147 ResNet/res3.2/h0_bn/gamma:0\n",
      "148 ResNet/res3.2/h0_bn/moving_mean:0\n",
      "149 ResNet/res3.2/h0_bn/moving_variance:0\n",
      "150 ResNet/res3.2/h1_conv/filter:0\n",
      "151 ResNet/res3.2/h1_conv/bias:0\n",
      "152 ResNet/res3.2/h1_bn/beta:0\n",
      "153 ResNet/res3.2/h1_bn/gamma:0\n",
      "154 ResNet/res3.2/h1_bn/moving_mean:0\n",
      "155 ResNet/res3.2/h1_bn/moving_variance:0\n",
      "156 ResNet/res3.2/h2_conv/filter:0\n",
      "157 ResNet/res3.2/h2_conv/bias:0\n",
      "158 ResNet/res3.3/h0_bn/beta:0\n",
      "159 ResNet/res3.3/h0_bn/gamma:0\n",
      "160 ResNet/res3.3/h0_bn/moving_mean:0\n",
      "161 ResNet/res3.3/h0_bn/moving_variance:0\n",
      "162 ResNet/res3.3/h1_conv/filter:0\n",
      "163 ResNet/res3.3/h1_conv/bias:0\n",
      "164 ResNet/res3.3/h1_bn/beta:0\n",
      "165 ResNet/res3.3/h1_bn/gamma:0\n",
      "166 ResNet/res3.3/h1_bn/moving_mean:0\n",
      "167 ResNet/res3.3/h1_bn/moving_variance:0\n",
      "168 ResNet/res3.3/h2_conv/filter:0\n",
      "169 ResNet/res3.3/h2_conv/bias:0\n",
      "170 ResNet/res3.4/h0_bn/beta:0\n",
      "171 ResNet/res3.4/h0_bn/gamma:0\n",
      "172 ResNet/res3.4/h0_bn/moving_mean:0\n",
      "173 ResNet/res3.4/h0_bn/moving_variance:0\n",
      "174 ResNet/res3.4/h1_conv/filter:0\n",
      "175 ResNet/res3.4/h1_conv/bias:0\n",
      "176 ResNet/res3.4/h1_bn/beta:0\n",
      "177 ResNet/res3.4/h1_bn/gamma:0\n",
      "178 ResNet/res3.4/h1_bn/moving_mean:0\n",
      "179 ResNet/res3.4/h1_bn/moving_variance:0\n",
      "180 ResNet/res3.4/h2_conv/filter:0\n",
      "181 ResNet/res3.4/h2_conv/bias:0\n",
      "182 ResNet/r4_bn/beta:0\n",
      "183 ResNet/r4_bn/gamma:0\n",
      "184 ResNet/r4_bn/moving_mean:0\n",
      "185 ResNet/r4_bn/moving_variance:0\n",
      "186 ResNet/fully_connected/weight:0\n",
      "187 ResNet/fully_connected/bias:0\n",
      "188 optimizer/Variable:0\n",
      "189 optimizer/ResNet/r0_conv/filter/Momentum:0\n",
      "190 optimizer/ResNet/r0_conv/bias/Momentum:0\n",
      "191 optimizer/ResNet/r0_bn/beta/Momentum:0\n",
      "192 optimizer/ResNet/r0_bn/gamma/Momentum:0\n",
      "193 optimizer/ResNet/res1.0/h1_conv/filter/Momentum:0\n",
      "194 optimizer/ResNet/res1.0/h1_conv/bias/Momentum:0\n",
      "195 optimizer/ResNet/res1.0/h1_bn/beta/Momentum:0\n",
      "196 optimizer/ResNet/res1.0/h1_bn/gamma/Momentum:0\n",
      "197 optimizer/ResNet/res1.0/h2_conv/filter/Momentum:0\n",
      "198 optimizer/ResNet/res1.0/h2_conv/bias/Momentum:0\n",
      "199 optimizer/ResNet/res1.1/h0_bn/beta/Momentum:0\n",
      "200 optimizer/ResNet/res1.1/h0_bn/gamma/Momentum:0\n",
      "201 optimizer/ResNet/res1.1/h1_conv/filter/Momentum:0\n",
      "202 optimizer/ResNet/res1.1/h1_conv/bias/Momentum:0\n",
      "203 optimizer/ResNet/res1.1/h1_bn/beta/Momentum:0\n",
      "204 optimizer/ResNet/res1.1/h1_bn/gamma/Momentum:0\n",
      "205 optimizer/ResNet/res1.1/h2_conv/filter/Momentum:0\n",
      "206 optimizer/ResNet/res1.1/h2_conv/bias/Momentum:0\n",
      "207 optimizer/ResNet/res1.2/h0_bn/beta/Momentum:0\n",
      "208 optimizer/ResNet/res1.2/h0_bn/gamma/Momentum:0\n",
      "209 optimizer/ResNet/res1.2/h1_conv/filter/Momentum:0\n",
      "210 optimizer/ResNet/res1.2/h1_conv/bias/Momentum:0\n",
      "211 optimizer/ResNet/res1.2/h1_bn/beta/Momentum:0\n",
      "212 optimizer/ResNet/res1.2/h1_bn/gamma/Momentum:0\n",
      "213 optimizer/ResNet/res1.2/h2_conv/filter/Momentum:0\n",
      "214 optimizer/ResNet/res1.2/h2_conv/bias/Momentum:0\n",
      "215 optimizer/ResNet/res1.3/h0_bn/beta/Momentum:0\n",
      "216 optimizer/ResNet/res1.3/h0_bn/gamma/Momentum:0\n",
      "217 optimizer/ResNet/res1.3/h1_conv/filter/Momentum:0\n",
      "218 optimizer/ResNet/res1.3/h1_conv/bias/Momentum:0\n",
      "219 optimizer/ResNet/res1.3/h1_bn/beta/Momentum:0\n",
      "220 optimizer/ResNet/res1.3/h1_bn/gamma/Momentum:0\n",
      "221 optimizer/ResNet/res1.3/h2_conv/filter/Momentum:0\n",
      "222 optimizer/ResNet/res1.3/h2_conv/bias/Momentum:0\n",
      "223 optimizer/ResNet/res1.4/h0_bn/beta/Momentum:0\n",
      "224 optimizer/ResNet/res1.4/h0_bn/gamma/Momentum:0\n",
      "225 optimizer/ResNet/res1.4/h1_conv/filter/Momentum:0\n",
      "226 optimizer/ResNet/res1.4/h1_conv/bias/Momentum:0\n",
      "227 optimizer/ResNet/res1.4/h1_bn/beta/Momentum:0\n",
      "228 optimizer/ResNet/res1.4/h1_bn/gamma/Momentum:0\n",
      "229 optimizer/ResNet/res1.4/h2_conv/filter/Momentum:0\n",
      "230 optimizer/ResNet/res1.4/h2_conv/bias/Momentum:0\n",
      "231 optimizer/ResNet/res2.0/h0_bn/beta/Momentum:0\n",
      "232 optimizer/ResNet/res2.0/h0_bn/gamma/Momentum:0\n",
      "233 optimizer/ResNet/res2.0/h1_conv/filter/Momentum:0\n",
      "234 optimizer/ResNet/res2.0/h1_conv/bias/Momentum:0\n",
      "235 optimizer/ResNet/res2.0/h1_bn/beta/Momentum:0\n",
      "236 optimizer/ResNet/res2.0/h1_bn/gamma/Momentum:0\n",
      "237 optimizer/ResNet/res2.0/h2_conv/filter/Momentum:0\n",
      "238 optimizer/ResNet/res2.0/h2_conv/bias/Momentum:0\n",
      "239 optimizer/ResNet/res2.1/h0_bn/beta/Momentum:0\n",
      "240 optimizer/ResNet/res2.1/h0_bn/gamma/Momentum:0\n",
      "241 optimizer/ResNet/res2.1/h1_conv/filter/Momentum:0\n",
      "242 optimizer/ResNet/res2.1/h1_conv/bias/Momentum:0\n",
      "243 optimizer/ResNet/res2.1/h1_bn/beta/Momentum:0\n",
      "244 optimizer/ResNet/res2.1/h1_bn/gamma/Momentum:0\n",
      "245 optimizer/ResNet/res2.1/h2_conv/filter/Momentum:0\n",
      "246 optimizer/ResNet/res2.1/h2_conv/bias/Momentum:0\n",
      "247 optimizer/ResNet/res2.2/h0_bn/beta/Momentum:0\n",
      "248 optimizer/ResNet/res2.2/h0_bn/gamma/Momentum:0\n",
      "249 optimizer/ResNet/res2.2/h1_conv/filter/Momentum:0\n",
      "250 optimizer/ResNet/res2.2/h1_conv/bias/Momentum:0\n",
      "251 optimizer/ResNet/res2.2/h1_bn/beta/Momentum:0\n",
      "252 optimizer/ResNet/res2.2/h1_bn/gamma/Momentum:0\n",
      "253 optimizer/ResNet/res2.2/h2_conv/filter/Momentum:0\n",
      "254 optimizer/ResNet/res2.2/h2_conv/bias/Momentum:0\n",
      "255 optimizer/ResNet/res2.3/h0_bn/beta/Momentum:0\n",
      "256 optimizer/ResNet/res2.3/h0_bn/gamma/Momentum:0\n",
      "257 optimizer/ResNet/res2.3/h1_conv/filter/Momentum:0\n",
      "258 optimizer/ResNet/res2.3/h1_conv/bias/Momentum:0\n",
      "259 optimizer/ResNet/res2.3/h1_bn/beta/Momentum:0\n",
      "260 optimizer/ResNet/res2.3/h1_bn/gamma/Momentum:0\n",
      "261 optimizer/ResNet/res2.3/h2_conv/filter/Momentum:0\n",
      "262 optimizer/ResNet/res2.3/h2_conv/bias/Momentum:0\n",
      "263 optimizer/ResNet/res2.4/h0_bn/beta/Momentum:0\n",
      "264 optimizer/ResNet/res2.4/h0_bn/gamma/Momentum:0\n",
      "265 optimizer/ResNet/res2.4/h1_conv/filter/Momentum:0\n",
      "266 optimizer/ResNet/res2.4/h1_conv/bias/Momentum:0\n",
      "267 optimizer/ResNet/res2.4/h1_bn/beta/Momentum:0\n",
      "268 optimizer/ResNet/res2.4/h1_bn/gamma/Momentum:0\n",
      "269 optimizer/ResNet/res2.4/h2_conv/filter/Momentum:0\n",
      "270 optimizer/ResNet/res2.4/h2_conv/bias/Momentum:0\n",
      "271 optimizer/ResNet/res3.0/h0_bn/beta/Momentum:0\n",
      "272 optimizer/ResNet/res3.0/h0_bn/gamma/Momentum:0\n",
      "273 optimizer/ResNet/res3.0/h1_conv/filter/Momentum:0\n",
      "274 optimizer/ResNet/res3.0/h1_conv/bias/Momentum:0\n",
      "275 optimizer/ResNet/res3.0/h1_bn/beta/Momentum:0\n",
      "276 optimizer/ResNet/res3.0/h1_bn/gamma/Momentum:0\n",
      "277 optimizer/ResNet/res3.0/h2_conv/filter/Momentum:0\n",
      "278 optimizer/ResNet/res3.0/h2_conv/bias/Momentum:0\n",
      "279 optimizer/ResNet/res3.1/h0_bn/beta/Momentum:0\n",
      "280 optimizer/ResNet/res3.1/h0_bn/gamma/Momentum:0\n",
      "281 optimizer/ResNet/res3.1/h1_conv/filter/Momentum:0\n",
      "282 optimizer/ResNet/res3.1/h1_conv/bias/Momentum:0\n",
      "283 optimizer/ResNet/res3.1/h1_bn/beta/Momentum:0\n",
      "284 optimizer/ResNet/res3.1/h1_bn/gamma/Momentum:0\n",
      "285 optimizer/ResNet/res3.1/h2_conv/filter/Momentum:0\n",
      "286 optimizer/ResNet/res3.1/h2_conv/bias/Momentum:0\n",
      "287 optimizer/ResNet/res3.2/h0_bn/beta/Momentum:0\n",
      "288 optimizer/ResNet/res3.2/h0_bn/gamma/Momentum:0\n",
      "289 optimizer/ResNet/res3.2/h1_conv/filter/Momentum:0\n",
      "290 optimizer/ResNet/res3.2/h1_conv/bias/Momentum:0\n",
      "291 optimizer/ResNet/res3.2/h1_bn/beta/Momentum:0\n",
      "292 optimizer/ResNet/res3.2/h1_bn/gamma/Momentum:0\n",
      "293 optimizer/ResNet/res3.2/h2_conv/filter/Momentum:0\n",
      "294 optimizer/ResNet/res3.2/h2_conv/bias/Momentum:0\n",
      "295 optimizer/ResNet/res3.3/h0_bn/beta/Momentum:0\n",
      "296 optimizer/ResNet/res3.3/h0_bn/gamma/Momentum:0\n",
      "297 optimizer/ResNet/res3.3/h1_conv/filter/Momentum:0\n",
      "298 optimizer/ResNet/res3.3/h1_conv/bias/Momentum:0\n",
      "299 optimizer/ResNet/res3.3/h1_bn/beta/Momentum:0\n",
      "300 optimizer/ResNet/res3.3/h1_bn/gamma/Momentum:0\n",
      "301 optimizer/ResNet/res3.3/h2_conv/filter/Momentum:0\n",
      "302 optimizer/ResNet/res3.3/h2_conv/bias/Momentum:0\n",
      "303 optimizer/ResNet/res3.4/h0_bn/beta/Momentum:0\n",
      "304 optimizer/ResNet/res3.4/h0_bn/gamma/Momentum:0\n",
      "305 optimizer/ResNet/res3.4/h1_conv/filter/Momentum:0\n",
      "306 optimizer/ResNet/res3.4/h1_conv/bias/Momentum:0\n",
      "307 optimizer/ResNet/res3.4/h1_bn/beta/Momentum:0\n",
      "308 optimizer/ResNet/res3.4/h1_bn/gamma/Momentum:0\n",
      "309 optimizer/ResNet/res3.4/h2_conv/filter/Momentum:0\n",
      "310 optimizer/ResNet/res3.4/h2_conv/bias/Momentum:0\n",
      "311 optimizer/ResNet/r4_bn/beta/Momentum:0\n",
      "312 optimizer/ResNet/r4_bn/gamma/Momentum:0\n",
      "313 optimizer/ResNet/fully_connected/weight/Momentum:0\n",
      "314 optimizer/ResNet/fully_connected/bias/Momentum:0\n",
      "Training stage 3: Epoch training...\n",
      "Training epoch 0, iteration 0, global_step 0, batch_loss 2.56457304955, batch_accuracy 0.0799999982119\n",
      "Training epoch 0, iteration 50, global_step 50, batch_loss 0.365296900272, batch_accuracy 0.990000009537\n",
      "Training epoch 0, iteration 100, global_step 100, batch_loss 0.348680824041, batch_accuracy 0.97000002861\n",
      "Training epoch 0, iteration 150, global_step 150, batch_loss 0.336930781603, batch_accuracy 0.97000002861\n",
      "Training epoch 0, iteration 200, global_step 200, batch_loss 0.322344243526, batch_accuracy 0.960000038147\n",
      "Training epoch 0, iteration 250, global_step 250, batch_loss 0.334693491459, batch_accuracy 0.980000019073\n",
      "Training epoch 0, iteration 300, global_step 300, batch_loss 0.261853963137, batch_accuracy 0.969999969006\n",
      "Training epoch 0, iteration 350, global_step 350, batch_loss 0.240523576736, batch_accuracy 1.0\n",
      "Training epoch 0, iteration 400, global_step 400, batch_loss 0.229253232479, batch_accuracy 1.0\n",
      "Training epoch 0, iteration 450, global_step 450, batch_loss 0.230802088976, batch_accuracy 1.0\n",
      "Training epoch 0, iteration 500, global_step 500, batch_loss 0.216485053301, batch_accuracy 1.0\n",
      "Training epoch 0, iteration 550, global_step 550, batch_loss 0.213036671281, batch_accuracy 1.0\n",
      "Testing epoch 0, loss 0.234348880053, error 0.0107999914885\n",
      "Training epoch 1, iteration 0, global_step 600, batch_loss 0.210889190435, batch_accuracy 1.0\n",
      "Training epoch 1, iteration 50, global_step 650, batch_loss 0.222170546651, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 100, global_step 700, batch_loss 0.214717850089, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 150, global_step 750, batch_loss 0.228263556957, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 200, global_step 800, batch_loss 0.214769080281, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 250, global_step 850, batch_loss 0.214540377259, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 300, global_step 900, batch_loss 0.202005416155, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 350, global_step 950, batch_loss 0.203552365303, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 400, global_step 1000, batch_loss 0.190272927284, batch_accuracy 1.0\n",
      "Training epoch 1, iteration 450, global_step 1050, batch_loss 0.209253877401, batch_accuracy 0.980000019073\n",
      "Training epoch 1, iteration 500, global_step 1100, batch_loss 0.19781216979, batch_accuracy 0.990000009537\n",
      "Training epoch 1, iteration 550, global_step 1150, batch_loss 0.169637903571, batch_accuracy 1.0\n",
      "Testing epoch 1, loss 0.196668093801, error 0.0109999936819\n",
      "Training epoch 2, iteration 0, global_step 1200, batch_loss 0.167977958918, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 50, global_step 1250, batch_loss 0.189803466201, batch_accuracy 0.990000009537\n",
      "Training epoch 2, iteration 100, global_step 1300, batch_loss 0.164392948151, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 150, global_step 1350, batch_loss 0.169082313776, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 200, global_step 1400, batch_loss 0.170047000051, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 250, global_step 1450, batch_loss 0.17684699595, batch_accuracy 0.990000009537\n",
      "Training epoch 2, iteration 300, global_step 1500, batch_loss 0.165054112673, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 350, global_step 1550, batch_loss 0.172621876001, batch_accuracy 0.990000009537\n",
      "Training epoch 2, iteration 400, global_step 1600, batch_loss 0.145242139697, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 450, global_step 1650, batch_loss 0.152016609907, batch_accuracy 1.0\n",
      "Training epoch 2, iteration 500, global_step 1700, batch_loss 0.186278611422, batch_accuracy 0.990000009537\n",
      "Training epoch 2, iteration 550, global_step 1750, batch_loss 0.166242077947, batch_accuracy 0.990000009537\n",
      "Testing epoch 2, loss 0.169041971862, error 0.0110999935865\n",
      "Training epoch 3, iteration 0, global_step 1800, batch_loss 0.141875207424, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 50, global_step 1850, batch_loss 0.149196118116, batch_accuracy 0.990000009537\n",
      "Training epoch 3, iteration 100, global_step 1900, batch_loss 0.137545600533, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 150, global_step 1950, batch_loss 0.135420411825, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 200, global_step 2000, batch_loss 0.135397478938, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 250, global_step 2050, batch_loss 0.140056505799, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 300, global_step 2100, batch_loss 0.136804029346, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 350, global_step 2150, batch_loss 0.15154466033, batch_accuracy 0.990000009537\n",
      "Training epoch 3, iteration 400, global_step 2200, batch_loss 0.180654659867, batch_accuracy 0.97000002861\n",
      "Training epoch 3, iteration 450, global_step 2250, batch_loss 0.144665524364, batch_accuracy 1.0\n",
      "Training epoch 3, iteration 500, global_step 2300, batch_loss 0.16926407814, batch_accuracy 0.990000009537\n",
      "Training epoch 3, iteration 550, global_step 2350, batch_loss 0.134040594101, batch_accuracy 1.0\n",
      "Testing epoch 3, loss 0.145607802421, error 0.00449999928474\n",
      "Training epoch 4, iteration 0, global_step 2400, batch_loss 0.133834436536, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 50, global_step 2450, batch_loss 0.132496759295, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 100, global_step 2500, batch_loss 0.134246721864, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 150, global_step 2550, batch_loss 0.13145108521, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 200, global_step 2600, batch_loss 0.134946450591, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 250, global_step 2650, batch_loss 0.132709488273, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 300, global_step 2700, batch_loss 0.13544934988, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 350, global_step 2750, batch_loss 0.136027932167, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 400, global_step 2800, batch_loss 0.131516084075, batch_accuracy 1.0\n",
      "Training epoch 4, iteration 450, global_step 2850, batch_loss 0.145107775927, batch_accuracy 0.989999949932\n",
      "Training epoch 4, iteration 500, global_step 2900, batch_loss 0.139654681087, batch_accuracy 0.989999949932\n",
      "Training epoch 4, iteration 550, global_step 2950, batch_loss 0.13116709888, batch_accuracy 1.0\n",
      "Testing epoch 4, loss 0.14177866444, error 0.00459999918938\n",
      "Training epoch 5, iteration 0, global_step 3000, batch_loss 0.134456112981, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 50, global_step 3050, batch_loss 0.130809187889, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 100, global_step 3100, batch_loss 0.14762635529, batch_accuracy 0.990000009537\n",
      "Training epoch 5, iteration 150, global_step 3150, batch_loss 0.184912323952, batch_accuracy 0.990000009537\n",
      "Training epoch 5, iteration 200, global_step 3200, batch_loss 0.128689438105, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 250, global_step 3250, batch_loss 0.133611321449, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 300, global_step 3300, batch_loss 0.159116119146, batch_accuracy 0.990000009537\n",
      "Training epoch 5, iteration 350, global_step 3350, batch_loss 0.146482229233, batch_accuracy 0.990000009537\n",
      "Training epoch 5, iteration 400, global_step 3400, batch_loss 0.129970431328, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 450, global_step 3450, batch_loss 0.150682762265, batch_accuracy 0.990000009537\n",
      "Training epoch 5, iteration 500, global_step 3500, batch_loss 0.128529518843, batch_accuracy 1.0\n",
      "Training epoch 5, iteration 550, global_step 3550, batch_loss 0.140319600701, batch_accuracy 0.990000009537\n",
      "Testing epoch 5, loss 0.137941809595, error 0.00379999935627\n",
      "Training epoch 6, iteration 0, global_step 3600, batch_loss 0.136681452394, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 50, global_step 3650, batch_loss 0.131538584828, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 100, global_step 3700, batch_loss 0.12895488739, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 150, global_step 3750, batch_loss 0.126144558191, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 200, global_step 3800, batch_loss 0.125745341182, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 250, global_step 3850, batch_loss 0.131669133902, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 300, global_step 3900, batch_loss 0.142246127129, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 350, global_step 3950, batch_loss 0.125880926847, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 400, global_step 4000, batch_loss 0.177348300815, batch_accuracy 0.980000019073\n",
      "Training epoch 6, iteration 450, global_step 4050, batch_loss 0.136403098702, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 500, global_step 4100, batch_loss 0.126663610339, batch_accuracy 1.0\n",
      "Training epoch 6, iteration 550, global_step 4150, batch_loss 0.127037212253, batch_accuracy 1.0\n",
      "Testing epoch 6, loss 0.137539055049, error 0.00389999985695\n",
      "Training epoch 7, iteration 0, global_step 4200, batch_loss 0.126558423042, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 50, global_step 4250, batch_loss 0.134402602911, batch_accuracy 0.990000009537\n",
      "Training epoch 7, iteration 100, global_step 4300, batch_loss 0.127446323633, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 150, global_step 4350, batch_loss 0.144069090486, batch_accuracy 0.990000009537\n",
      "Training epoch 7, iteration 200, global_step 4400, batch_loss 0.135453954339, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 250, global_step 4450, batch_loss 0.133395791054, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 300, global_step 4500, batch_loss 0.130700811744, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 350, global_step 4550, batch_loss 0.131089061499, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 400, global_step 4600, batch_loss 0.127662822604, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 450, global_step 4650, batch_loss 0.125683948398, batch_accuracy 1.0\n",
      "Training epoch 7, iteration 500, global_step 4700, batch_loss 0.141368746758, batch_accuracy 0.990000009537\n",
      "Training epoch 7, iteration 550, global_step 4750, batch_loss 0.125525087118, batch_accuracy 1.0\n",
      "Testing epoch 7, loss 0.137299018577, error 0.00369999885559\n",
      "Training epoch 8, iteration 0, global_step 4800, batch_loss 0.130467310548, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 50, global_step 4850, batch_loss 0.130510523915, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 100, global_step 4900, batch_loss 0.127050489187, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 150, global_step 4950, batch_loss 0.126049146056, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 200, global_step 5000, batch_loss 0.139547854662, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 250, global_step 5050, batch_loss 0.125495612621, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 300, global_step 5100, batch_loss 0.178576648235, batch_accuracy 0.990000009537\n",
      "Training epoch 8, iteration 350, global_step 5150, batch_loss 0.133173972368, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 400, global_step 5200, batch_loss 0.127578228712, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 450, global_step 5250, batch_loss 0.125562250614, batch_accuracy 1.0\n",
      "Training epoch 8, iteration 500, global_step 5300, batch_loss 0.140244886279, batch_accuracy 0.990000009537\n",
      "Training epoch 8, iteration 550, global_step 5350, batch_loss 0.126704499125, batch_accuracy 1.0\n",
      "Testing epoch 8, loss 0.136987022087, error 0.00359999895096\n",
      "Training epoch 9, iteration 0, global_step 5400, batch_loss 0.125536844134, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 50, global_step 5450, batch_loss 0.190748721361, batch_accuracy 0.980000019073\n",
      "Training epoch 9, iteration 100, global_step 5500, batch_loss 0.128636851907, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 150, global_step 5550, batch_loss 0.125854104757, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 200, global_step 5600, batch_loss 0.134062290192, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 250, global_step 5650, batch_loss 0.168116986752, batch_accuracy 0.990000009537\n",
      "Training epoch 9, iteration 300, global_step 5700, batch_loss 0.126031070948, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 350, global_step 5750, batch_loss 0.124624244869, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 400, global_step 5800, batch_loss 0.184192746878, batch_accuracy 0.980000019073\n",
      "Training epoch 9, iteration 450, global_step 5850, batch_loss 0.124640576541, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 500, global_step 5900, batch_loss 0.127085357904, batch_accuracy 1.0\n",
      "Training epoch 9, iteration 550, global_step 5950, batch_loss 0.125337764621, batch_accuracy 1.0\n",
      "Testing epoch 9, loss 0.136780522987, error 0.00359999895096\n",
      "Training epoch 10, iteration 0, global_step 6000, batch_loss 0.170316100121, batch_accuracy 0.990000009537\n",
      "Training epoch 10, iteration 50, global_step 6050, batch_loss 0.127204835415, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 100, global_step 6100, batch_loss 0.14489531517, batch_accuracy 0.990000009537\n",
      "Training epoch 10, iteration 150, global_step 6150, batch_loss 0.19025953114, batch_accuracy 0.990000009537\n",
      "Training epoch 10, iteration 200, global_step 6200, batch_loss 0.125220239162, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 250, global_step 6250, batch_loss 0.127743914723, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 300, global_step 6300, batch_loss 0.137864217162, batch_accuracy 0.990000009537\n",
      "Training epoch 10, iteration 350, global_step 6350, batch_loss 0.126083135605, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 400, global_step 6400, batch_loss 0.125019520521, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 450, global_step 6450, batch_loss 0.126688927412, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 500, global_step 6500, batch_loss 0.128410816193, batch_accuracy 1.0\n",
      "Training epoch 10, iteration 550, global_step 6550, batch_loss 0.140757203102, batch_accuracy 0.990000009537\n",
      "Testing epoch 10, loss 0.136366531104, error 0.00370000004768\n",
      "Training epoch 11, iteration 0, global_step 6600, batch_loss 0.124917313457, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 50, global_step 6650, batch_loss 0.137695759535, batch_accuracy 0.990000009537\n",
      "Training epoch 11, iteration 100, global_step 6700, batch_loss 0.137475371361, batch_accuracy 0.990000009537\n",
      "Training epoch 11, iteration 150, global_step 6750, batch_loss 0.124369852245, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 200, global_step 6800, batch_loss 0.140754401684, batch_accuracy 0.990000009537\n",
      "Training epoch 11, iteration 250, global_step 6850, batch_loss 0.147815674543, batch_accuracy 0.990000009537\n",
      "Training epoch 11, iteration 300, global_step 6900, batch_loss 0.125110447407, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 350, global_step 6950, batch_loss 0.162040382624, batch_accuracy 0.980000019073\n",
      "Training epoch 11, iteration 400, global_step 7000, batch_loss 0.127062663436, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 450, global_step 7050, batch_loss 0.128803864121, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 500, global_step 7100, batch_loss 0.127295926213, batch_accuracy 1.0\n",
      "Training epoch 11, iteration 550, global_step 7150, batch_loss 0.12797921896, batch_accuracy 1.0\n",
      "Testing epoch 11, loss 0.136042136326, error 0.00369999885559\n",
      "Training epoch 12, iteration 0, global_step 7200, batch_loss 0.134607464075, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 50, global_step 7250, batch_loss 0.136729508638, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 100, global_step 7300, batch_loss 0.125950589776, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 150, global_step 7350, batch_loss 0.126535579562, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 200, global_step 7400, batch_loss 0.128672704101, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 250, global_step 7450, batch_loss 0.125021651387, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 300, global_step 7500, batch_loss 0.149065881968, batch_accuracy 0.990000009537\n",
      "Training epoch 12, iteration 350, global_step 7550, batch_loss 0.124516837299, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 400, global_step 7600, batch_loss 0.124351516366, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 450, global_step 7650, batch_loss 0.128940805793, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 500, global_step 7700, batch_loss 0.124530985951, batch_accuracy 1.0\n",
      "Training epoch 12, iteration 550, global_step 7750, batch_loss 0.130392000079, batch_accuracy 1.0\n",
      "Testing epoch 12, loss 0.135960970744, error 0.00369999945164\n",
      "Training epoch 13, iteration 0, global_step 7800, batch_loss 0.128400444984, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 50, global_step 7850, batch_loss 0.124829508364, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 100, global_step 7900, batch_loss 0.130245536566, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 150, global_step 7950, batch_loss 0.129481121898, batch_accuracy 0.999999940395\n",
      "Training epoch 13, iteration 200, global_step 8000, batch_loss 0.126448646188, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 250, global_step 8050, batch_loss 0.13439977169, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 300, global_step 8100, batch_loss 0.12424505502, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 350, global_step 8150, batch_loss 0.125062823296, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 400, global_step 8200, batch_loss 0.125456750393, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 450, global_step 8250, batch_loss 0.132846057415, batch_accuracy 0.990000009537\n",
      "Training epoch 13, iteration 500, global_step 8300, batch_loss 0.12737852335, batch_accuracy 1.0\n",
      "Training epoch 13, iteration 550, global_step 8350, batch_loss 0.139375731349, batch_accuracy 0.990000009537\n",
      "Testing epoch 13, loss 0.135913768113, error 0.00379999876022\n",
      "Training epoch 14, iteration 0, global_step 8400, batch_loss 0.134017571807, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 50, global_step 8450, batch_loss 0.124297648668, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 100, global_step 8500, batch_loss 0.125982746482, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 150, global_step 8550, batch_loss 0.182916820049, batch_accuracy 0.989999949932\n",
      "Training epoch 14, iteration 200, global_step 8600, batch_loss 0.123845763505, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 250, global_step 8650, batch_loss 0.125955179334, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 300, global_step 8700, batch_loss 0.142585039139, batch_accuracy 0.990000009537\n",
      "Training epoch 14, iteration 350, global_step 8750, batch_loss 0.124897316098, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 400, global_step 8800, batch_loss 0.125009536743, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 450, global_step 8850, batch_loss 0.125077366829, batch_accuracy 1.0\n",
      "Training epoch 14, iteration 500, global_step 8900, batch_loss 0.161430895329, batch_accuracy 0.990000009537\n",
      "Training epoch 14, iteration 550, global_step 8950, batch_loss 0.127337947488, batch_accuracy 1.0\n",
      "Testing epoch 14, loss 0.135847495273, error 0.00379999876022\n",
      "Training epoch 15, iteration 0, global_step 9000, batch_loss 0.125724568963, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 50, global_step 9050, batch_loss 0.127557009459, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 100, global_step 9100, batch_loss 0.129994168878, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 150, global_step 9150, batch_loss 0.125493720174, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 200, global_step 9200, batch_loss 0.132335394621, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 250, global_step 9250, batch_loss 0.124371834099, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 300, global_step 9300, batch_loss 0.170585215092, batch_accuracy 0.980000019073\n",
      "Training epoch 15, iteration 350, global_step 9350, batch_loss 0.128879621625, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 400, global_step 9400, batch_loss 0.125411584973, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 450, global_step 9450, batch_loss 0.123758658767, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 500, global_step 9500, batch_loss 0.128069490194, batch_accuracy 1.0\n",
      "Training epoch 15, iteration 550, global_step 9550, batch_loss 0.134185358882, batch_accuracy 0.990000009537\n",
      "Testing epoch 15, loss 0.135797198266, error 0.00379999935627\n",
      "Training epoch 16, iteration 0, global_step 9600, batch_loss 0.138857349753, batch_accuracy 0.990000009537\n",
      "Training epoch 16, iteration 50, global_step 9650, batch_loss 0.124281652272, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 100, global_step 9700, batch_loss 0.129682153463, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 150, global_step 9750, batch_loss 0.130505979061, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 200, global_step 9800, batch_loss 0.129923850298, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 250, global_step 9850, batch_loss 0.125442028046, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 300, global_step 9900, batch_loss 0.12751865387, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 350, global_step 9950, batch_loss 0.127985998988, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 400, global_step 10000, batch_loss 0.139436244965, batch_accuracy 0.990000009537\n",
      "Training epoch 16, iteration 450, global_step 10050, batch_loss 0.124644234776, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 500, global_step 10100, batch_loss 0.126754283905, batch_accuracy 1.0\n",
      "Training epoch 16, iteration 550, global_step 10150, batch_loss 0.143626436591, batch_accuracy 0.990000009537\n",
      "Testing epoch 16, loss 0.135729690343, error 0.00369999885559\n",
      "Training epoch 17, iteration 0, global_step 10200, batch_loss 0.128021255136, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 50, global_step 10250, batch_loss 0.129032850266, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 100, global_step 10300, batch_loss 0.128389179707, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 150, global_step 10350, batch_loss 0.124596871436, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 200, global_step 10400, batch_loss 0.124583363533, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 250, global_step 10450, batch_loss 0.124229058623, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 300, global_step 10500, batch_loss 0.124853491783, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 350, global_step 10550, batch_loss 0.134447440505, batch_accuracy 0.990000009537\n",
      "Training epoch 17, iteration 400, global_step 10600, batch_loss 0.127007812262, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 450, global_step 10650, batch_loss 0.124088078737, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 500, global_step 10700, batch_loss 0.126028224826, batch_accuracy 1.0\n",
      "Training epoch 17, iteration 550, global_step 10750, batch_loss 0.129777833819, batch_accuracy 1.0\n",
      "Testing epoch 17, loss 0.135667470321, error 0.00370000004768\n",
      "Training epoch 18, iteration 0, global_step 10800, batch_loss 0.128690242767, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 50, global_step 10850, batch_loss 0.127683103085, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 100, global_step 10900, batch_loss 0.148650571704, batch_accuracy 0.990000009537\n",
      "Training epoch 18, iteration 150, global_step 10950, batch_loss 0.129808947444, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 200, global_step 11000, batch_loss 0.124451510608, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 250, global_step 11050, batch_loss 0.125267624855, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 300, global_step 11100, batch_loss 0.124197684228, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 350, global_step 11150, batch_loss 0.125293955207, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 400, global_step 11200, batch_loss 0.12433077395, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 450, global_step 11250, batch_loss 0.125233396888, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 500, global_step 11300, batch_loss 0.124157086015, batch_accuracy 1.0\n",
      "Training epoch 18, iteration 550, global_step 11350, batch_loss 0.124023281038, batch_accuracy 1.0\n",
      "Testing epoch 18, loss 0.135605654269, error 0.00379999995232\n",
      "Training epoch 19, iteration 0, global_step 11400, batch_loss 0.132651180029, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 50, global_step 11450, batch_loss 0.124451935291, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 100, global_step 11500, batch_loss 0.135307580233, batch_accuracy 0.990000009537\n",
      "Training epoch 19, iteration 150, global_step 11550, batch_loss 0.129584252834, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 200, global_step 11600, batch_loss 0.123891174793, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 250, global_step 11650, batch_loss 0.134991019964, batch_accuracy 0.989999949932\n",
      "Training epoch 19, iteration 300, global_step 11700, batch_loss 0.1281106323, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 350, global_step 11750, batch_loss 0.124037586153, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 400, global_step 11800, batch_loss 0.124024532735, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 450, global_step 11850, batch_loss 0.132018446922, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 500, global_step 11900, batch_loss 0.130288645625, batch_accuracy 1.0\n",
      "Training epoch 19, iteration 550, global_step 11950, batch_loss 0.124983325601, batch_accuracy 1.0\n",
      "Testing epoch 19, loss 0.135547171906, error 0.00379999876022\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "# Create computation graph.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Set GPU number and train.\n",
    "    gpu_number = 1\n",
    "    with tf.device(\"/gpu:{0}\".format(gpu_number)):    \n",
    "        # Training session.\n",
    "        with tf.Session(config = config) as sess:\n",
    "            train(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
